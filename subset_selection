#----------------------------------------------------------
# Subset Selection: Best subset, Forward, Backward, Hybrid
# Regularization: Ridge regression, LASSO
# Dimension Reduction: PCR, PLS
#----------------------------------------------------------

#----------
# 1. Subset Selection

# 1-1. Best Subset Selection
install.packages("ISLR")
library(ISLR)
Hitters <- Hitters
str(Hitters)
names(Hitters)
dim(Hitters)

# removes all of the rows that have missing values in any variable
sum(is.na(Hitters))
Hitters <- na.omit(Hitters)
sum(is.na(Hitters))

dim(Hitters)

table(Hitters$League)
table(Hitters$Division)
table(Hitters$League, Hitters$Division)

# linear regression model using lm() function
lm.fit <- lm(Salary ~ ., data = Hitters)
summary(lm.fit)

# best subset selection
install.packages("leaps")
library(leaps)
regfit.full <- regsubsets(Salary ~ ., data = Hitters) # restricted to 8 variables
summary(regfit.full)

# nvmax option to increase # of variables
regfit.full <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19) 
reg.summary <- summary(regfit.full)
names(reg.summary)

# statistics from reg.summary
reg.summary$rsq

# plotting RSS, adjusted R2, Cp, BIC
par(mfrow = c(2,2))
plot(reg.summary$rss, xlab = "Number of Variables", ylab = "RSS", type = "l")

plot(reg.summary$adjr2, xlab = "Number of Variables", ylab = "Adjusted RSq", type = "l")
which.max(reg.summary$adjr2)
points(11, reg.summary$adjr2[11], col = "red", cex = 2, pch = 20)

plot(reg.summary$cp, xlab = "Number of Variables", ylab = "Cp", type = "l")
which.min(reg.summary$cp)
points(10, reg.summary$cp[10], col = "red", cex = 2, pch = 20)

plot(reg.summary$bic, xlab = "Number of Variables", ylab = "BIC", type = "l")
which.min(reg.summary$bic)
points(6, reg.summary$bic[6], col = "red", cex = 2, pch = 20)

par(mfrow = c(1,1))
plot(regfit.full, scale = "r2")
plot(regfit.full, scale = "adjr2")
plot(regfit.full, scale = "Cp")
plot(regfit.full, scale = "bic")

# see the coefficient estimates associated with best subset selection model
coef(regfit.full, 6)


# ----------
# Forward and Backward Stepwise Selection

# Forward Stepwise Selection
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
summary(regfit.fwd)
reg.fwd.summary <- summary(regfit.fwd)
which.min(reg.fwd.summary$cp) # 10

# Backward Stepwise Selection
regfit.bwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "backward")
summary(regfit.bwd)
reg.bwd.summary <- summary(regfit.bwd)
which.min(reg.bwd.summary$bic)

# Sequence replacement search
regfit.seqrep <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "seqrep")
summary(regfit.seqrep)
reg.seqrep.summary <- summary(regfit.seqrep)
which.min(reg.seqrep.summary$bic)


# ----------
# Choosing among models using the validaton set approach and Cross-Validation

# splitting the observations into a training set and a test set
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(Hitters), rep = TRUE)
test <- (!train)

# apply regsubsets() to the training set in order to perform best subset selection
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters[train,], nvmax = 19, method = "forward")

# make a model matrix from the test data: "X" matrix form data
test.mat <- model.matrix(Salary ~ ., data = Hitters[test,])

# run a loop, and for each size i, we extract the coefficents from regfit.fwd
# multiply coefficients into the appropriate columns of the test model matrix to form the predictions, 
# and compute the test MSE
val.errors <- rep(NA, 19)
for (i in 1:19){
  coefi <- coef(regfit.fwd, id = i) # extracting coefficients
  pred <- test.mat[, names(coefi)]%*%coefi # prediction of test set
  val.errors[i] <- mean((Hitters$Salary[test] - pred)^2) # MSE of test set
}

val.errors
which.min(val.errors)
coef(regfit.fwd, 12)

# make predict method
predict.regsubsets <- function(object, newdata, id, ...){
  form <- as.formula(object$call[[2]])
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars]%*%coefi
}

# make regression model with full data set
regfit.fwd <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19, method = "forward")
coef(regfit.fwd, 10)


# ----------
# Cross-Validation

k <- 10
set.seed(1)
folds <- sample(1:k, nrow(Hitters), replace = TRUE)
cv.errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))

for (j in 1:k) {
  best.fit <- regsubsets(Salary ~ ., data = Hitters[folds !=j,], nvmax = 19)
  for (i in 1:19) {
    pred <- predict(best.fit, Hitters[folds == j,], id = i)
    cv.errors[j, i] <- mean((Hitters$Salary[folds == j] - pred)^2)
  }
}

# plot of MSE per number of variables
mean.cv.errors <- apply(cv.errors, 2, mean)
mean.cv.errors
plot(mean.cv.errors, type = "b")
which.min(mean.cv.errors) # 11
points(11, mean.cv.errors[11], col = "red", cex = 2, pch=20)

# perform best subset selection on the full data set in order to obtain the 11-variable model
reg.best <- regsubsets(Salary ~ ., data = Hitters, nvmax = 19)
coef(reg.best, 11)


# ----------
# Ridge Regression
install.packages("glmnet")
library(glmnet)

x <- model.matrix(Salary ~ ., data = Hitters)[,-1]
y <- Hitters$Salary

grid <- 10^seq(10, -2, length = 100)
ridge.mod <- glmnet(x, y, alpha = 0, lambda = grid) # alpha = 0 : Ridge Regression, standardization is default

dim(coef(ridge.mod))

plot(ridge.mod)

ridge.mod$lambda[50]
coef(ridge.mod)[,50]

ridge.mod$lambda[60]
coef(ridge.mod)[,60]

# obtain the ridge regression coefficients for a new value of lambda, say 50 by using predict()
predict(ridge.mod, s=50, type = "coefficients")[1:20,]
barplot(predict(ridge.mod, s=50, type = "coefficients")[1:20,])


# sampling of train, test set
set.seed(1)
train <- sample(1:nrow(x), nrow(x)/2)
test <- (-train)
y.test <- y[test]

ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = grid, thresh = 1e-12) # alpha = 0: Ridge
ridge.pred <- predict(ridge.mod, s=4, newx = x[test,])
mean((ridge.pred - y.test)^2) # MSE of test set

# choosing the best lambda by cross-validation
set.seed(1)
cv.out <- cv.glmnet(x[train,], y[train], alpha=0)
plot(cv.out)
bestlam <- cv.out$lambda.min # best lambda
bestlam

ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,])
mean((ridge.pred - y.test)^2)


# ----------
# LASSO(Least Absolute Shrinkage and Selection Operator)
lasso.mod <- glmnet(x[train,], y[train], alpha = 1, lambda = grid)
plot(lasso.mod)

# choosing the best lambda by Cross-Validation
set.seed(1)
cv.out.lasso <- cv.glmnet(x[train,], y[train], alpha=1) # alpha=1: LASSO
plot(cv.out.lasso)
bestlam <- cv.out.lasso$lambda.min
bestlam
lasso.pred <- predict(lasso.mod, s=bestlam, newx=x[test,])
mean((lasso.pred - y.test)^2)

lasso.mod.full <- glmnet(x, y, alpha=1, lambda=grid)
lasso.coef <- predict(lasso.mod.full, type = "coefficients", s=bestlam)[1:20,]
lasso.coef
lasso.coef[lasso.coef != 0]
