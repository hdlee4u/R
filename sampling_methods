##---------------------------------------------------
## Resampling Methods
## Cross-Validation and the Bootstrap
## Reference: An introduction to statistical learning 
##            with application in R
##---------------------------------------------------

# loading library and data
install.packages("ISLR")
library(ISLR)
Auto <- Auto
str(Auto)

#-----------
# 1-1. random sampling
set.seed(1)
train <- sample(x=392, size=196, replace=FALSE)

# (1-1-1) linear regression modeling
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset=train)
summary(lm.fit)

attach(Auto)
mean((mpg - predict(lm.fit, Auto))[-train]^2) # MSE of validation set
#mean((mpg[-train] - predict(lm.fit, Auto[-train,]))^2) # MSE of validation set

# (1-1-2) polynomial regression modeling
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto, subset=train) 
summary(lm.fit2)
mean((mpg - predict(lm.fit2, Auto))[-train]^2) # MSE of validation set

# (1-1-3) cubic regression modeling
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data=Auto, subset=train)
summary(lm.fit3)
mean((mpg - predict(lm.fit3, Auto))[-train]^2) # MSE of validation set

detach(Auto)


#-----------
# 1-2random sampling with different set
set.seed(2)
train <- sample(392, 196, replace=FALSE)

# (1-2-1) linear regression modeling
lm.fit <- lm(mpg ~ horsepower, data=Auto, subset=train)
mean((mpg - predict(lm.fit, Auto))[-train]^2)

# (1-2-2) polynomial regression modeling
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data=Auto, subset=train)
mean((mpg - predict(lm.fit2, Auto))[-train]^2)

# (1-2-3) cubic regression modeling
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data=Auto, subset=train)
mean((mpg - predict(lm.fit3, Auto))[-train]^2)


#-----------
# 2. LOOCV (Leave-One-Out Cross-Validation)
library(boot)
glm.fit <- glm(mpg ~ horsepower, data=Auto)
cv.err <- cv.glm(Auto, glm.fit)
names(cv.err)
cv.err$delta # cross-validation results
cv.err$K # 392 all

# LOOCV by polynomials of order i=1 to i=5
cv.error <- rep(0, 5) # initialize the ev.error vector with '0'

for (i in 1:5) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data=Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}

cv.error
plot(cv.error, main="Cross-Validation Error", xlab="polynomial order")


#----------
# 3. k-Fold Cross-Validation

set.seed(17)
cv.error.10 <- rep(0, 10) # initialize the 10-fold cv.error

for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data=Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K=10)$delta[1] # 10-fold CV (K=10)
}

cv.error.10
plot(cv.error.10)
which.min(cv.error.10)


#----------
# 4. The Bootstrap
# : repeatedly sampling observations from the data set with replacement

str(Portfolio)
head(Portfolio)

alpha.fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  return((var(Y) - cov(X, Y))/(var(X) + var(Y) - 2*cov(X, Y)))
}

alpha.fn(Portfolio, 1:100)

# Bootstrap with sample(replace=T) function
set.seed(1)
alpha.fn(Portfolio, sample(100, 100, replace=TRUE))

# Bootstrap with boot() function
boot(data=Portfolio, 
     statistic=alpha.fn, 
     R=1000) # 1000 bootstrp estimates for alpha

# estimating the accuracy of a linear regression model
boot.fn <- function(data, index) {
  return(coef(lm(mpg ~ horsepower, data=data, subset=index)))
}

boot.fn(Auto, 1:392)

set.seed(1)
boot.fn(Auto, sample(392, 392, replace=T))
boot.fn(Auto, sample(392, 392, replace=T))


# computing the standard errors of 1,000 bootstrap estimates 
# for the intercept and slope terms
boot(data=Auto, statistic=boot.fn, R=1000)

# standard errors by using summary() function
summary(lm(mpg ~ horsepower, data=Auto))$coef # using formula w/ assumptions


# computing the SE of 1,000 bootstrap estimates w/ polynomial regresstion
boot.fn <- function(data, index) {
  coef(lm(mpg ~ horsepower + I(horsepower^2), 
          data=data, 
          subset=index))
}

set.seed(1)
boot(data=Auto, statistic=boot.fn, R=1000)

'ORDINARY NONPARAMETRIC BOOTSTRAP


Call:
boot(data = Auto, statistic = boot.fn, R = 1000)


Bootstrap Statistics :
        original        bias     std. error
t1* 56.900099702 -6.863131e-02 2.1133492036
t2* -0.466189630  9.942586e-04 0.0336689369
t3*  0.001230536 -3.381623e-06 0.0001218251'
